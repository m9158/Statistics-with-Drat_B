# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `2부-데이터 분석 준비하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다


## Statistics_7th_TIL

### 2부. 데이터 분석 준비하기

### 11. 데이터 전처리와 파생변수 생성

<!-- 11.5 모델 성능 향상을 위한 파 변수 생성부터 11장 끝까지 진행해주시면 됩니다.-->

## Study ScheduleStudy Schedule

| 주차  | 공부 범위     | 완료 여부 |
| ----- | ------------- | --------- |
| 1주차 | 1부 p.2~46    | ✅         |
| 2주차 | 1부 p.47~81   | ✅         |
| 3주차 | 2부 p.82~120  | ✅         |
| 4주차 | 2부 p.121~167 | ✅         |
| 5주차 | 2부 p.168~202 | ✅         |
| 6주차 | 2부 p.203~250 | ✅         |
| 7주차 | 2부 p.251~299 | ✅         |

<!-- 여기까진 그대로 둬 주세요-->



---

# 1️⃣ 개념 정리 

## 11.데이터 전처리와 파생변수 생성

```
✅ 학습 목표 :
* 결측값과 이상치를 식별하고 적절한 방법으로 처리할 수 있다.
* 데이터 변환과 가공 기법을 학습하고 활용할 수 있다.
* 모델 성능 향상을 위한 파생 변수를 생성하고 활용할 수 있다.
```

### 11.5. 모델 성능 향상을 위한 파생 변수 생성

1. 파생변수란?
   - 원래 있던 변수들을 조합하거나 함수를 적용하여 새로 만들어낸 변수를 뜻한다.
   - 데이터 구간화, 표준화 및 정규화 등도 일종의 파생변수라 할 수 있다.
   - 그 밖에 기존 값에 로그나 제곱근 등을 취해 변동성을 완화시키거나 지수함수를 사용하여 분산을 증폭시킬수도 있다
   - 파생변수는 데이터의 특성을 이용하여 분석 효율을 높이는 것이기 때문에 전체 데이터에 파악이 중요할 뿐만 아니라 해당 비즈니스 도메인에 대한
     충분한 이해가 수반되어야 한다.

2. 파생변수 사용의 유의점
   - 기존의 변수를 활용해서 만들어 낸 변수이기 때문에 다중공선성 문제가 발생할 가능성이 높다.
   - 그렇기 때문에 파생변수를 만든 다음에는 상관분석을 통해 변수 간의 상관성을 확인 해야한다.
   - 상관성에 따라 파생변수를 그대로 사용할지, 기존 변수를 제외하고 파생변수만 사용할지 여부를 결정해야 한다.
  
3. 파생 변수 생성 실습


<img width="1133" height="417" alt="image" src="https://github.com/user-attachments/assets/8909cf74-4aac-4e18-82d1-4918e4db8b91" />

~~~
- 두 개 이상의 변수 간에 사칙연산을 적용하여 새로운 파생변수를 생성
- Sales_Amount 칼럼이 총 구매 금액이라고 가정 했을 때 상품 하나당 가격을 의미하는 칼럼을 생성하려면 Quantity 칼럼으로 나눠 주면 됨.
- Sales_Amount 칼럼이 상품 하나당 가격을 의미하는 칼럼일 경우, Sales_Amount 칼럼과 Quantity 칼럼을 곱해주면 된다.
~~~


<img width="1728" height="528" alt="image" src="https://github.com/user-attachments/assets/8a9f8977-0597-42d2-b30f-aa2d80cf012d" />


~~~
- 변수의 분산을 조정하기 위한 로그, 제곱근, 제곱 등의 함수를 적용함.
- 로그를 취할 때는 0값은 처리가 되지 않기 때문에 1을 더해주고 로그를 취하거나 임의의 값으로 바꿔줘야함.
~~~


<img width="1783" height="718" alt="image" src="https://github.com/user-attachments/assets/4f981a82-7cbd-47f6-989f-ac2a0892d376" />


~~~
- 기존 데이터는 일별 구매 정보가 기록
- 따라서 월별 총 구매금액, 평균 구매금액 파생변수를 생성하기 위해서는 연월 기준으로 계산해줘야함.
- 따라서 고객별 월별 매출 합계와 매출 평균 테이블을 생성한 다음 기존 테이블에 조인
~~~


<img width="1768" height="572" alt="image" src="https://github.com/user-attachments/assets/fea97b95-056c-43b2-8ddd-ae7f22c08e5d" />


~~~
- 월평균 구매금액 대비 차이를 계산한 변수는 구매금액 수준이 평소에 얼마나 높거나 낮아졌는지 알 수 있기 때문에 이탈 감지 변수로 활용 가능하다.
~~~


<img width="1746" height="275" alt="image" src="https://github.com/user-attachments/assets/695ddab3-2bbd-4ef5-b783-c2f71a27591e" />


~~~
- 전 달의 총 구매금액, 평균 구매금액 파생변수 생성.
- 이를 위해서 기존 시점 칼럼인 Date2를 한 달 뒤 시점으로 변환한 뒤에 Sales_Amount의 합계 평균을 구한 다음 기존 한 달 전 시점 칼럼을 기준으로 조인해줘야 한다.
- 만약 전 달의 값이 없는 경우에는 결측값이 생길 수 있기 대문에 결측값은 0으로 처리해줘야 한다.
~~~


<img width="1773" height="432" alt="image" src="https://github.com/user-attachments/assets/674df489-a119-4818-b09e-2df818c5dd29" />


~~~
- 기준 시점과 한 달 전 시점 간의 차이를 계산하면 고객의 성향이 어떻게 변했는지 판단할 수 있는 변수를 만들 수 있다.
- 전월 대비 구매금액 평균 차이 변수는 고객의 객단가가 어떻게 변했는지를 나타내기 때문에 이탈이나 우수고객 예측에 사용될 수 있다.
- 전월 대비 총 구매금액 차이 변수는 고객이 이번달 남은 기간 동안 얼마나 더 구매할 것인지 예측할 때 사용할 수 있다.
~~~

<!-- 새롭게 배운 내용을 자유롭게 정리해주세요. -->

### 11.6. 슬라이딩 윈도우 데이터 가공

1. 슬라이딩 윈도우란?
   - 본래 실시간 네트워크 패킷 데이터를 처리하는 기법이다.
   - 현재 시점으로 부터 +-M 기간의 데이터를 일정 간격의 시간마다 전송하는 방식으로,
     이 방식의 특징은 데이터 조각(Window)들이 서로 겹치며 데이터가 전송되는 것이다.
   - 데이터를 겹쳐 나눔으로써 전체 데이터가 증가하는 원리를 차용한 것이다.

2. 슬라이딩 윈도우 데이터 가공 사용 예시 (예측 모델)
   - 인터넷 쇼밍몰에서 고객의 지난 5개월 간의 구매내역, 방문 횟수 등의 데이터를 활용하여 한 달간의 재구매 확률을 구하는 모델
   - 1년치의 데이터가 있을 경우, 일반적인 방법으로는 6개월을 학습 셋, 나머지 6개월은 테스트 셋으로 데이터를 구성하여 예측 분석 모델을 만든다.
   - 그런데 구매 내역 데이터가 충분하지 않을 경우 예측력이 좋은 모델을 만들기가 어려움
   - 또한 학습 시기와 예측 시기와의 시점 차이가 큼
     
     =>이런 경우 슬라이딩 윈도우 방법을 활용하면 많은 분석 데이터셋을 확보하고 학습데이터의 최근성을 가질 수 있게됨.

3. 슬라이딩 윈도우 실습

<img width="850" height="599" alt="image" src="https://github.com/user-attachments/assets/18365923-dd19-406d-9738-06c99e891809" />

~~~
- 슬라우딩 윈도 형태로 변환하기 앞서, 각 날짜가 기준 시점으로부터 얼마나 차이가 나는지를 알 수 있는 칼럼 생성.
- 예를 들어 첫번째 행의 2021년 12월 9일은 기준으로 잡은 2022년 5월 1일로부터는 5개월 전(YM_M0 = M5)이다.
- 그리고 2022년 5월 1일로부터 한 달 전 기준으로 4개월 전(YM_M1 = M4)이다.
~~~


<img width="995" height="222" alt="image" src="https://github.com/user-attachments/assets/3599ce6a-81af-4131-a7d6-d94675638989" />


~~~
- 슬라이딩 윈도우 가공된 테이블 생성
- 각 고객의 기준 시점별 구매금액이 월별로 집계 됨.
- MM_DIFF 칼럼도 해당 연월로 대체하여 사용할 수 있다.
~~~


<img width="996" height="375" alt="image" src="https://github.com/user-attachments/assets/0261e0d7-b7bd-45f5-b170-28fc4039d128" />


~~~
- 슬라이딩 윈도우가 제대로 적용됐는지 검증하기 위해 특정 고객만 추출하여 테이블 형태를 확인
- 기준 시점이 2022년 5월일 경우에는 6달전 구매금액이 39,200원이다.
- 그리고 1개월씩 과거 시점으로 갈수록 구매 금액 시점도 한 칼럼씩 옮겨 졌다,
~~~


<img width="944" height="430" alt="image" src="https://github.com/user-attachments/assets/3b25bc99-a429-42c6-ae47-fb8b86a30eb9" />



~~~
- 마지막 시점의 데이터 확인.
- Sale_amt_M0 값이 있고 나머지 칼럼은 0값을 가지고 있다.
- 이러한 이유는 기존 데이터가 6개월 동안의 구매 데이터만 존재하기 때문이다.
- 마지막 시점인 2021년 12월의 1~5개월 전까지의 데이터가 있어야만 온전한 데이터 활용이 가능하다.
- 이처럼 슬라이딩 윈도우는 필요한 기간을 먼저 고려하고 활용해야 한다.
~~~


<!-- 새롭게 배운 내용을 자유롭게 정리해주세요. -->

### 11.7. 범주형 변수의 가변수 처리

1. 가변수 처리란?
   - 범주형 변수를 0과 1의 값을 가지는 변수로 변환해 주는 것을 뜻한다.
   - 범주형 변수는 사용할 수 없고 연속형 변수만 사용 가능한 분석 기법을 사용하기 위함이다.
   - 이런 형태는 이진변수라고도 하며, 불리언 변수라고도 한다.
  
2. 가변수의 활용
   - 가변수가 범주의 수보다 하나 적게 만들어야한다.
   - 회귀식에서 기울기를 바꾸는 것이 아닌 절편값을 바꾸는 값이 된다.
   - 이는 효율성 뿐만아니라 가변수 처리를 하는 것은 기존 하나의 변수를 여러 개의 변수로 나눠준 것이기 대문에 각각의
     변수는 독립성을 가져야하기 때문이다. 이는 다중공선성을 피하기 위함이다.

3. 범주형 변수의 가변수 처리 실습


<img width="779" height="755" alt="image" src="https://github.com/user-attachments/assets/393420e4-11dd-497e-b0c3-fae22c1c7d6b" />


~~~
- Compute Type과 OS 칼럼을 가변수 처리하기에 앞서 각 범주의 분포를 확인해 본다.
- 전자는 5개에 CPU 범주의 비율이 가장 많으며, 후자는 세 개의 범주로 Windows OS 범주가 가장 많은 비율을 차지한다.
~~~


<img width="1805" height="553" alt="image" src="https://github.com/user-attachments/assets/c35756d2-6244-4342-b8d1-ad08f888bf09" />



~~~
- 판다스의 가변수 처리 함수인 get_dimmies()를 그대로 적용하면 문자형 변수를 모두 가변수 처리한다.
- 그렇기 때문에 Device Name 칼럼까지 처리가 되어 740개나 되는 칼럼을 생성했다.
- 본인은 True와 False값으로 나오길래 int 형으로 변경해주는 절차를 추가로 넣었다.
~~~


<img width="1541" height="368" alt="image" src="https://github.com/user-attachments/assets/5ab01ab8-6029-44e7-8296-41365bc5d3c9" />

~~~
- 앞의 문제를 방지하기 위해 두 칼럼만 지정하여 가변수 처리 한다.
- 각 칼럼명에 범주명이 붙은 칼럼명이 자동으로 만들어진다.
- 각 칼럼은 해당하면 1, 해당되지 않으면 0으로 채워진다.
~~~


<img width="1395" height="347" alt="image" src="https://github.com/user-attachments/assets/eba2816c-54ea-4170-a942-bbc6bf499fa8" />


~~~
- 다중공선성 문제를 방지하기 위해 하나의 범주 칼럼을 제거해줘야 한다.
- drop_first = True 옵션을 설정하면 자동으로 첫 번째 범주를 제거해 준다.
- 아웃풋 테이블을 보면 Compute Type 칼럼은 네 개. OS 칼럼은 두 개로 하나씩 줄어든 것을 확인할 수 있다.
~~~


<img width="1538" height="361" alt="image" src="https://github.com/user-attachments/assets/8f95cd3a-0a8c-4b01-b8ef-fca5ae485453" />



~~~
- 결측값이 있는 상태로 가변수 처리를 할 때는 결측값을 별도의 범주로 처리하는 옵션을 적용하는 것이 좋다.
- 범주형 값이 결측값인 것 자체가 의미가 있을 수 있으므로 dummy_na = True 옵션을 적용하여 결측값 범주를 생성.
- 아웃풋 테이블에 '_nan'접미사가 붙은 칼럼들이 생성됨을 알 수 있다.
~~~

<!-- 새롭게 배운 내용을 자유롭게 정리해주세요. -->

### 11.8. 클래스 불균형 문제 해결을 위한 언더샘플링과 오버샘플링

1. 데이터 불균형 문제를 해결하는 두가지 종류

   1-1. 가중치 밸런싱
     - 모델 자체에 중요도가 높은 클래스에 정확도 가중치를 주어, 특정 클래스의 분류 정확도가 높아지도록 하는 것.
  
   1-2. 불균형 데이터 자체를 균형이 맞도록 가공하는 방법

     1-2-1. 언더샘플링
       - 큰 비중의 클래스 데이터를 작은 비중의 클래스 데이터만큼만 추출하여 학습시키는 것

     1-2-2. 오버 샘플링
       - 작은 비중의 클래스 데이터를 큰 비중의 클래스 데이터 만큼 늘려 추출하여 학습시키는 것.

3. 언더샘플링과 오버샘플링 실습
   

<img width="808" height="653" alt="image" src="https://github.com/user-attachments/assets/1846d09b-ea0f-4025-a190-8fb047b455c3" />


~~~
- 클래스 비율을 조정해야 하는 Purchased 칼럼의 기존 분포 시각화
- 총 400개의 관측치 중 0값은 약 250개, 1값은 약 150개 가량, 1값이 전체의 약 35% 정도로 약간의 클래스 불균형 존재 확인.
~~~


<img width="743" height="431" alt="image" src="https://github.com/user-attachments/assets/fa0f5484-007b-427c-b558-ac199de678ee" />


~~~
- 문자형 변수였던 Gender 칼럼을 가변수로 처리
- 모델에서는 남성과 여성 칼럼을 모두 넣을 필요가 없으므로 남성 여부 칼럼만 넣어줌
~~~


<img width="566" height="455" alt="image" src="https://github.com/user-attachments/assets/1ad6dc48-40b6-4889-a590-1fb5072eadd1" />



~~~
- 테스트셋은 실제 데이터에 대한 모델 성능을 평가하는 용도로 사용하는 것이기 때문에 학습셋에만 언더샘플링이나 오버샘플링을 적용한다.
~~~


<img width="1769" height="574" alt="image" src="https://github.com/user-attachments/assets/b70d7fd2-8850-4483-bc7d-af8c4f1cfdb7" />



~~~
- RandomUnderSampler() 함수를 사용하여 학습셋에 언더샘플링을 적용한다.
- 기존 300개의 관측치가 112:112의 224개의 관측치로 감소함
~~~


<img width="803" height="642" alt="image" src="https://github.com/user-attachments/assets/ec68b246-4c92-410e-ba64-c1e69f7fd68d" />


~~~
- 시각화를 통해 두개의 클래스의 분포가 동일한 것을 확인
~~~


<img width="1776" height="558" alt="image" src="https://github.com/user-attachments/assets/7cb9e98b-f22f-4825-8308-a5636a0a6ee6" />


~~~
- SMOTE() 함수를 적용하여 학습셋에 오버샘플링을 적용
- 기존 300개의 관측치가 188:188의 376개의 관측치로 증가
~~~


<img width="760" height="648" alt="image" src="https://github.com/user-attachments/assets/239d3756-c9d8-433a-9dff-9c8a9e6b93e3" />


~~~
- 오버 샘플링을 적용한 후의 클래스 분포를 시각화하면 두 클래스가 동일한 것을 확인 가능
~~~



<!-- 새롭게 배운 내용을 자유롭게 정리해주세요. -->

### 11.9. 데이터 거리 측정 방법

1. 데이터간의 거리란?
   - X, Y 축의 2차원 좌표가 있다고 해보자
   - "(1,1)에 위치한 관측치 A, (1,3)에 위치한 관측치 B, (2,5)에 위치한 관측치 C가 있다.
   - 관측치 A를 기준으로, B와 C중 어느 관측치가 더 가까이 있는가를 판단하기 위한 것이 데이터 거리 측정이다.
   - 공간상 데이터들 간의 거리가 가까우면 가까울수록 유사하다고 볼 수 있다.
   - 다만 데이터 거리를 측정하기 전에 데이터 표준화나 정규화 가공을 해줘야 한다.

2. 대표적인 거리 측정 방법

   2-1. 유클리드 거리
     - 데이터 거리 측정의 가장 대표적인 알고리즘은 유클리드 거리 측정
     - 피타고라스 정리를 활용한 값이다. 즉, 관측치 간의 거리를 측정하는 것
     - 유클리드 거리 값이 0에 가까울수록 데이터 간의 거리가 짧다는 것이므로, 유사도가 높음을 의미한다.

   2-2. 맨하튼 거리
     - 체계적인 도시 계획으로 구성된 맨해튼의 격자 모양 도로에서 최단 거리를 구하는 원리를 활용.
     - 맨하튼 거리는 L1 Norm이라 불리며, L2 Norm은 유클리드 거리다.
     - 보통 딥러닝 분야에서 정규화를 할 때 L1 Norm, L2 Norm이라는 용어로 데이터(백터) 간 거리를 구한다.
     - A 지점과 B 지점까지의 X 축거리, Y축 거리를 합해주는 값이다.

   2-3. 민코프스키 거리
     - 유클리드 거리 수식과 동일하지만 단지 제곱 부분을 p-norm값으로 조정할 수 있다.
     - p값을 1로 설정하면 맨해튼 거리와 동일하고, 2로 설정하면 유클리드 거리와 동일하다.
     - p값은 반드시 1 이상이어야 하고, 정수가 아니어도 상관없다.

   2-4. 체비쇼프 거리
    - 민코프스키 거리의 p 값을 무한대로 설정했을 때, 체비쇼프 거리 혹은 맥시멈 거리라고 한다. (또는 L max Norm)
    - 일반적으로는 잘 사용하지 않지만, 때에 따라서 군집 간 최대 거리가 중요한 경우에 사용한다.

   2-5. 마할라노비스 거리
     - 유클리드 거리에 공분산을 고려한 거리 측정 방법이다.
     - 변수 내 분산과 변수 간 공분산을 모두 반영하여 A와 B 간 거리를 계산한다.
     - 따라서 단순 거리에 상관성을 함께 볼 수 있다는 장점이 있다.
     - 유클리드 거리는 단순 직선 거리를 통해 거리를 측정하지만, 마할라노비스 거리는 두 지점의 공분산을 고려하여 거리를 측정한다.
     - 마할라노비스 거리는 확률 분포를 고려하기 때문에 공분산 행렬을 사용한다.
     - X가 증가할 때 Y도 함께 증가하는 상관성을 고려하여, 해당 점이 상관성으로 예측 가능한 영역에 있으면, 그만큼 거리가 단축된다.
     - 반대로 해당 점이 상관성과 관계없는 좌표에 위치해 있으면, 그 거리는 실제 거리보다 멀게 측정된다.
  
   2-6. 코사인 거리
     - 유클리드 거리는 줄자로 두 점 사이의 거리를 재는 것과 같은 개념이라면,
     - 코사인 유사도는 벡터 사이의 각도만으로 두 점 간의 유사도를 측정한다.
     - 즉, 두 벡터의 사이각을 구해서 유사도를 구하는 것이다.
     - 두 점 간의 각도가 작으면 유사도가 높고, 각도가 크면 유사도가 낮아지는 것이다.
     - 즉, 실제 거리보다, 좌표 공간상 각도가 얼마나 차이나는가에 따라 유사도를 측정한다.
     - 코사인 유사도는 -1에서 1사이의 값을 가지며, 두 벡터의 방향이 완전히 동일하면 1의 값을 가진다.
     - 코사인 유사도는 추천 시스템 중 하나인 협업 필터링 모델이나 문서 간 유사도를 측정하는 모델에서 좋은 성능을 보여준다.
     - 즉, 변수 간 스케일이 아닌, 각 관측치의 스케일이 다를 때 코사인 유사도가 좋은 성능을 낸다.
     - 1에서 코사인 유사도를 빼 주면 코사인 거리이다. 따라서 유사도가 높을수록 거리는 줄어드는 방식이다.


3. 데이터 거리 측정 실습

<img width="1232" height="628" alt="image" src="https://github.com/user-attachments/assets/ceee5933-9efa-40f8-9786-c6b69a14f6bc" />


~~~
- 고급 수학 함수, 수치적 미적분, 미분 방정식 계산, 최적화, 신호 처리 등에 사용되는 scipy 패키지에서는 소개한 데이터 거리 측정 방법을 모두 지원한다.
- 따라서 metric 옵션만 조정해 주면 원하는 방식으로 거리를 측정할 수 있다.
- 본 예시에서는 'euclidean' 옵션을 넣어 스타벅스 지점 간의 유클리드 거리를 산출했다.
~~~


<img width="1165" height="641" alt="image" src="https://github.com/user-attachments/assets/387a7772-e0e1-4ff6-b7ba-05ec415c6b1b" />


~~~
- 맨해튼 거리 측정은 metric 옵션에서 'cityblock'을 설정해준다.
- 테이블 형태는 상관성 분석의 상관계수 행렬과 유사하다.
- 동일한 지점은 거리가 없기 때문에 0으로 나오며, 다른 모든 지점과의 거리가 측정되어 나타난다.
~~~


<img width="1158" height="630" alt="image" src="https://github.com/user-attachments/assets/7a828b7b-6344-46b0-91ee-2d921550ae54" />


~~~
- 민코프스키 거리는 p값을 1로 설정하면 맨해튼 거리와 동일하고, 2로 설정하면 유클리드 거리와 동일하다.
- 예시에서는 1.5로 설정하여 거리를 측정했다.
~~~


<img width="1192" height="601" alt="image" src="https://github.com/user-attachments/assets/422f9db9-e2b9-452b-975e-fe74e16b82da" />


~~~
- 체피쇼프 거리는 metric 옵션에 'chebyshev'를 설정해준다.
~~~



<img width="1165" height="611" alt="image" src="https://github.com/user-attachments/assets/00200e49-cc83-4f23-95ce-2d4cb135ec6c" />


~~~
-  마할라노비스 거리는 metric 옵션에 'mahalanobis'를 설정해준다.
~~~



<img width="1305" height="641" alt="image" src="https://github.com/user-attachments/assets/c3953656-9126-4134-bdbf-a3946c250156" />


~~~
- 코사인 거리는 metric 옵션에 'cosine'을 설정해준다.
~~~

<!-- 새롭게 배운 내용을 자유롭게 정리해주세요. -->

<br>
<br>

---

# 2️⃣ 확인 과제

> **교재에 있는 실습 파트를 직접 따라 해보세요. 실습을 완료한 뒤, 결과화면(캡처 또는 코드 결과)을 첨부하여 인증해 주세요.**
>
> **단순 이론 암기보다, 직접 손으로 따라해보면서 실습해 보는 것이 가장 확실한 학습 방법입니다.**
>
> > **인증 예시 : 통계 프로그램 결과, 시각화 이미지 캡처 등**

<!-- 이 주석을 지우고 “실습 결과 화면(캡처)을 이곳에 첨부해주세요.-->

~~~
인증 이미지가 없으면 과제 수행으로 인정되지 않습니다.
~~~



---

# 3️⃣ 실습 과제 (마지막 과제)

>  **🧚Q. 마지막 과제는 다음과 같습니다. 『데이터 분석가가 반드시 알아야 할 모든 것』 2부를 마무리하는 주차로,그동안 배운 데이터 전처리 및 파생변수 생성 내용을 실제 데이터에 적용해 보는 실습형 과제입니다. 단순히 함수를 실행하는 데서 그치지 않고, "왜 이 전처리 방법을 선택했는가" 와 "데이터가 말해주는 인사이트는 무엇인가'를 중심으로 EDA(탐색적 데이터 분석)를 함께 수행해주세요.**
>
> (정규과제 업로드 시트에 과제를 수행한 Git 링크와 코랩도 같이 올려주세요) 

<!-- 4주차 과제부터 실습하면서 배운 파이썬 문법을 적용하면서 실습을 진행해주세요 -->

~~~
과제 가이드라인

1. 실습 데이터셋 불러오기
Kaggle : Students Performance in Exams
- 출처: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams
- 설명:
미국 고등학생 1000명의 성적과 배경 요인(성별, 인종, 부모 학력, 점심 여부, 시험 준비 과정 등)을 담은 데이터입니다.
math score, reading score, writing score 3가지 점수를 기준으로
학업 성취에 영향을 미치는 요인을 분석해볼 수 있습니다.

2. 데이터 전처리 진행하기
교재에서 배웠던 개념들을 적용해면서 전처리를 진행해봅시다. 
- 결측값 처리, 이상치 처리, 스케일링 등 
- (Optional) 범주형 변수 인코딩, 파생 변수 생성

3. EDA (탐색적 데이터 분석)
전처리된 데이터를 바탕으로 자유롭게 시각화 및 요약 분석을 수행하세요. 
- 점수 간 상관관계 분석
- 그룹 별 비교
- 여러 과정에 따른 성적 분포 비교
- 변수 간 관계 시각화 

4. 주석이나 코드 설명에서 들어가야 할 부분
- 교재에 있는 어떤 통계 개념을 적용했는지
- 각 개념이 데이터 분석에서 어떻게 활용되었는지를 스스로 설명해보세요.
- 단순한 코드 작성보다, 통계 개념 -> 코드 적용 -> 해석 -> 배운 점의 흐름을 명확히 드러내는 것이 핵심 기준입니다. 
~~~



<!-- 이것으로 통계학 정규과제가 마무리 되었습니다.  자료실에서 보면 아시겠지만, 이번 통계학 정규과제는 2부까지만 진행을 하였습니다. 3부부터는 모델에 대한 개념이 등장하기 때문에, 수학적 통계학을 배우고 분석의 기초를 다지는 부분에 여러분이 더 집중할 수 있도록 구성했습니다. 또한 전체 분량이 길기 때문에 학습 부담을 줄이기 위한 결정입니다. 따라서 이번 주차를 끝으로 정규 과제는 마무리되지만, 머신러닝 모델에 대해 더 깊이 공부하고 싶은 분들은 3부를 개인적으로 학습해보는 것을 추천드립니다. 그동안 과제를 열심히 하느라 고생하셨습니다. -->

### 🎉 수고하셨습니다.
